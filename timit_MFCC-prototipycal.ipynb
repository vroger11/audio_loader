{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vroger/.miniconda3/envs/audio_loader/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from pytorch_lightning.metrics.functional import accuracy, confusion_matrix\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from threadpoolctl import threadpool_info, threadpool_limits \n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import RandomApply, Compose\n",
    "\n",
    "from audio_loader.features.raw_audio import WindowedAudio\n",
    "from audio_loader.features.mfcc import WindowedMFCC\n",
    "from audio_loader.ground_truth.timit import TimitGroundTruth\n",
    "from audio_loader.samplers.dynamic_sampler import DynamicSamplerFromGt\n",
    "from audio_loader.dl_frontends.pytorch.fill_ram import pad_collate_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lib in threadpool_info(): \n",
    "    threadpool_limits(limits=max(2, int(lib['num_threads']/6)), user_api=lib['user_api'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_func(p=0.):\n",
    "    return Compose([\n",
    "        RandomApply([transforms.TimeMasking(2)], p=p),\n",
    "        RandomApply([transforms.FrequencyMasking(1)], p=p)\n",
    "    ])\n",
    "\n",
    "class TrainValidTestPrototypicalDataset(Dataset):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            data: list of tuples (data, gt)\n",
    "            anchor_data: list of tuples (data, gt) being the anchors\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the set times the number of anchors used.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a tuple of (data, anchor ) corresponding to the index.\"\"\"\n",
    "        datum = self.data[index]\n",
    "        return datum[0], datum[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimitMFCCDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, batch_size, shots=5, queries=15, batch_size_test=256):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_test = batch_size_test\n",
    "        self.shots = shots\n",
    "        self.queries = queries\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.timit_gt = TimitGroundTruth(self.data_dir, with_silences=False, phon_class=\"phon_class2\", return_original_gt=True)\n",
    "        self.timit_gt.set_gt_format(phonetic=True, word=False, speaker_id=True)\n",
    "        self.mfcc_feature_processor = WindowedMFCC(400, 160, 16000, 13, delta_orders=[1, 2], delta_width=9)\n",
    "        self.sampler = DynamicSamplerFromGt([self.mfcc_feature_processor], self.timit_gt)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            data_train = [(torch.tensor(x[0]),\n",
    "                           y[:-1].argmax(),\n",
    "                           y_label,\n",
    "                           y[-1],\n",
    "                           self.timit_gt.index2speaker_id[int(y[-1])][0],\n",
    "                           self.timit_gt.df_all[self.timit_gt.df_all['speaker_id'] == self.timit_gt.index2speaker_id[int(y[-1])]][\"dialect_region\"].values[0])\n",
    "                          for x, (y, y_label) in self.sampler.get_samples_from(\"train\", randomize_files=False)]\n",
    "            # select the anchors, the rest will be used for valid data\n",
    "            self.df_data_train = pd.DataFrame(data_train, columns=['datum', 'gt', 'original_label', 'speaker_id', \"sex\", 'region']) \n",
    "            all_speaker_id = self.df_data_train['speaker_id'].unique()\n",
    "            all_unique_gt = self.df_data_train['gt'].unique()\n",
    "            \n",
    "            dict_type_per_class = {}\n",
    "            for gt in all_unique_gt:\n",
    "                dict_type_per_class[gt] = self.df_data_train[self.df_data_train['gt'] == gt]['original_label'].unique()\n",
    "                \n",
    "            all_original_labels = self.df_data_train['original_label'].unique()\n",
    "            speaker_used_overall = [[] for i in range(self.number_of_classes)]\n",
    "\n",
    "            self.anchor_data, self.queries_data, speaker_used = self.select_samples(data_train,\n",
    "                                                                                  self.shots,\n",
    "                                                                                  speaker_used=speaker_used_overall,\n",
    "                                                                                  region_constraint=False,\n",
    "                                                                                   dict_type_per_class=dict_type_per_class)\n",
    "            \n",
    "            self.queries_data, self.valid_data, speaker_used = self.select_samples(self.queries_data,\n",
    "                                                                                   self.queries,\n",
    "                                                                                   speaker_used=speaker_used_overall,\n",
    "                                                                                   region_constraint=False,\n",
    "                                                                                   dict_type_per_class=dict_type_per_class)\n",
    "            self.train_dataset = TrainValidTestPrototypicalDataset(self.queries_data)\n",
    "            \n",
    "            # take less few samples for the valid set to avoid too long computation time\n",
    "            i_classes = np.zeros(self.number_of_classes)\n",
    "            examples_per_class_valid = 20\n",
    "    \n",
    "            self.valid_data, self.nu_data, speaker_used = self.select_samples(self.valid_data,\n",
    "                                                                              examples_per_class_valid,\n",
    "                                                                              speaker_used=speaker_used_overall,\n",
    "                                                                              region_constraint=False,\n",
    "                                                                             dict_type_per_class=dict_type_per_class)\n",
    "            \n",
    "            self.valid_dataset = TrainValidTestPrototypicalDataset(self.valid_data)\n",
    "            return\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            data_test = [(torch.tensor(x[0]),\n",
    "                               y[:-1].argmax(),\n",
    "                               y[-1],\n",
    "                               self.timit_gt.index2speaker_id[int(y[-1])][0],\n",
    "                               self.timit_gt.df_all[self.timit_gt.df_all['speaker_id'] == self.timit_gt.index2speaker_id[int(y[-1])]][\"dialect_region\"].values[0])\n",
    "                              for x, (y, y_label) in self.sampler.get_samples_from(\"test\")]\n",
    "            self.df_data_test = pd.DataFrame(data_test, columns=['datum', 'gt', 'speaker_id', \"sex\", 'region']) \n",
    "            self.test_dataset = TrainValidTestPrototypicalDataset(data_test)\n",
    "            return\n",
    "\n",
    "    \n",
    "    def select_samples(self, data, shots, speaker_used, region_constraint=False, dict_type_per_class=None):\n",
    "        \"\"\"Return a tuple with anchors, unused data and the speaker id used per class.\"\"\"\n",
    "        \n",
    "        anchor_data = []\n",
    "        # initialise counters\n",
    "        number_of_sample_selected = np.zeros(self.number_of_classes)\n",
    "        male_per_phon = np.zeros(self.number_of_classes)\n",
    "        female_per_phon = np.zeros(self.number_of_classes)\n",
    "        limit_female = round(shots / 2.)\n",
    "        limit_male = shots - limit_female\n",
    "        \n",
    "        dict_limit_per_type = {}\n",
    "        dict_type_count = {}\n",
    "        for label in dict_type_per_class:\n",
    "            limit = math.ceil(shots/len(dict_type_per_class[label]))\n",
    "            for original_label in dict_type_per_class[label]:\n",
    "                dict_limit_per_type[original_label] = limit\n",
    "                dict_type_count[original_label] = 0\n",
    "        \n",
    "\n",
    "        # region_used = [[] for i in range(self.number_of_classes)]\n",
    "        region_used = np.zeros((self.number_of_classes, 8)) # 8 number of regions \n",
    "        region_goal = np.zeros((self.number_of_classes, 8)) # 8 number of regions\n",
    "        for i in range(shots):\n",
    "            region_goal[:, i%8] += 1\n",
    "\n",
    "        if region_constraint:\n",
    "            # fix as there is no example from R8 in some classes\n",
    "            # don't work for shots > 10\n",
    "            if self.number_of_classes == 58:\n",
    "                region_goal[29, 7] = 0\n",
    "                region_goal[50, 7] = 0\n",
    "                region_goal[51, 7] = 0\n",
    "                region_goal[29, 2] += 1\n",
    "                region_goal[50, 2] += 1\n",
    "                region_goal[51, 2] += 1\n",
    "\n",
    "                if region_goal[50, 0] > 1: \n",
    "                    region_goal[50, 0] = 1\n",
    "                    region_goal[51, 0] = 1\n",
    "                    region_goal[50, 4] += 1\n",
    "                    region_goal[51, 4] += 1\n",
    "\n",
    "            elif self.number_of_classes == 47:\n",
    "                # don't work for shots > 10\n",
    "                region_goal[40, 7] -= 0\n",
    "                region_goal[40, 0] -= 0\n",
    "                region_goal[40, 2] += 1\n",
    "                region_goal[40, 3] += 1\n",
    "            elif self.number_of_classes == 39:\n",
    "                # don't work for shots > 10\n",
    "                region_goal[33, 7] -= 0\n",
    "                region_goal[33, 0] -= 0\n",
    "                region_goal[33, 2] += 1\n",
    "                region_goal[33, 3] += 1\n",
    "\n",
    "        i = 0\n",
    "        number_anchors = 0\n",
    "        expected_anchors = shots * self.number_of_classes\n",
    "        # apply strategies to select data\n",
    "        while i < len(data) and number_anchors < expected_anchors:\n",
    "            datum, gt, original_gt, speaker_id, sex, region = data[i]\n",
    "            if region_constraint:\n",
    "                condition = number_of_sample_selected[gt] < shots and speaker_id not in speaker_used[gt] and region_used[gt, int(region[-1])-1] < region_goal[gt, int(region[-1])-1] and dict_type_count[original_gt] < dict_limit_per_type[original_gt]\n",
    "            else:\n",
    "                condition = number_of_sample_selected[gt] < shots and speaker_id not in speaker_used[gt] and dict_type_count[original_gt] < dict_limit_per_type[original_gt]\n",
    "                \n",
    "            if condition:\n",
    "                if sex == \"F\":\n",
    "                    if female_per_phon[gt] < limit_female:\n",
    "                        female_per_phon[gt] += 1\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                else:\n",
    "                    if female_per_phon[gt] < limit_male:\n",
    "                        male_per_phon[gt] += 1\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                region_used[gt, int(region[-1])-1] += 1\n",
    "                number_of_sample_selected[gt] += 1\n",
    "                dict_type_count[original_gt] += 1\n",
    "                speaker_used[gt].append(speaker_id)\n",
    "                number_anchors += 1\n",
    "                anchor_data.append((datum, gt))\n",
    "                del data[i]\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        if number_anchors < expected_anchors:\n",
    "            print(region_goal-region_used)\n",
    "            print(region_goal)\n",
    "            print(region_used)\n",
    "            print(f\"number_of_sample_selected:\\n{number_of_sample_selected}\")\n",
    "            print(f\"speaker_used:\\n{speaker_used}\")\n",
    "            raise Exception(\"strategy not possible\")\n",
    "\n",
    "        return anchor_data, data, speaker_used\n",
    "    \n",
    "    @property\n",
    "    def number_of_classes(self):\n",
    "        return self.sampler.number_of_classes-1  # minus one as the last dimension is for the speaker id\n",
    "    \n",
    "    @property\n",
    "    def anchors(self):\n",
    "        return self.anchor_data\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if len(self.train_dataset) < self.batch_size:\n",
    "            raise Exception(f\"Error the dataset size ({len(self.train_dataset)}) if inferior to the desired batch size ({self.batch_size})\")\n",
    "    \n",
    "        data_loader =  DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                                  collate_fn=pad_collate_supervised,\n",
    "                                  num_workers=1,\n",
    "                                  drop_last=True)\n",
    "        print(f\"train {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        data_loader = DataLoader(self.valid_dataset, batch_size=self.batch_size_test, shuffle=False,\n",
    "                                 collate_fn=pad_collate_supervised,\n",
    "                                 num_workers=0,\n",
    "                                 drop_last=False)\n",
    "        print(f\"valid {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        data_loader = DataLoader(self.test_dataset, self.batch_size_test , shuffle=False,\n",
    "                                 collate_fn=pad_collate_supervised,\n",
    "                                 num_workers=0,\n",
    "                                 drop_last=False)\n",
    "        print(f\"test {len(data_loader)}\")\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lit_mfcc_model(pl.LightningModule):\n",
    "    def __init__(self, feature_size, anchor_data, num_classes, num_shots, labels):\n",
    "        \"\"\"Init all parameters.\n",
    "        \n",
    "        feature_size: int\n",
    "            size of the expected features for the forward step\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.num_classes = num_classes\n",
    "        self.num_shots = num_shots\n",
    "        self.labels = labels\n",
    "        \n",
    "        # prepare the anchors for test and valid\n",
    "        self.anchors_gt = [gt for data, gt in anchor_data]\n",
    "        anchors = [data for data, gt in anchor_data]\n",
    "        \n",
    "        # sort anchors\n",
    "        sorted_idx = sorted(range(len(self.anchors_gt)), key=lambda k: self.anchors_gt[k])\n",
    "        self.anchors_gt = [self.anchors_gt[idx] for idx in sorted_idx]\n",
    "        anchors = [anchors[idx] for idx in sorted_idx]\n",
    "        \n",
    "        \n",
    "        # prepare anchor data\n",
    "        anchors_lens = [len(x) for x in anchors]\n",
    "        anchors_padded = pad_sequence(anchors, batch_first=True, padding_value=0.)\n",
    "        self.packed_anchors = pack_padded_sequence(anchors_padded, anchors_lens,\n",
    "                                                   batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Initialize the model\n",
    "        self.layer_1_grus = nn.GRU(\n",
    "            feature_size, 256, 5,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        self.bn_fwd = nn.BatchNorm1d(256)\n",
    "        self.bn_bwd = nn.BatchNorm1d(256)\n",
    "        self.layer_2_dense = torch.nn.Linear(512, 128)\n",
    "        \n",
    "        self.alpha = torch.nn.Linear(128, 128, bias=False)\n",
    "        \n",
    "        self.loss = torch.nn.CrossEntropyLoss() #torch.nn.BCEWithLogitsLoss(size_average=True)\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        batch_size = x.batch_sizes[0]\n",
    "        # shape: (num_layers*directions, batch_size, hidden_size?)\n",
    "        h_0 = torch.zeros(5*2, batch_size, 256, device=self.device)\n",
    "        output, h_n = self.layer_1_grus(x, h_0)\n",
    "\n",
    "        fwd_h = h_n.view(5, 2, batch_size, 256)[-1, 0]\n",
    "        bwd_h = h_n.view(5, 2, batch_size, 256)[-1, 1]\n",
    "    \n",
    "        fwd_h = self.bn_fwd(fwd_h.view(batch_size, 256))\n",
    "        bwd_h = self.bn_bwd(bwd_h.view(batch_size, 256))\n",
    "\n",
    "        h = torch.cat((fwd_h, bwd_h), 1)\n",
    "        return self.layer_2_dense(h)\n",
    "        \n",
    "\n",
    "    def compute_prototypes(self):\n",
    "        cks = [[] for i in range(len(np.unique(self.anchors_gt)))]\n",
    "        \n",
    "        out = self.forward_one(self.packed_anchors.to(self.device))\n",
    "        for anchor_emb, gt in zip(out, self.anchors_gt):\n",
    "            cks[gt].append(anchor_emb)\n",
    "            \n",
    "        for i in range(len(cks)):\n",
    "            cks[i] = torch.mean(torch.stack(cks[i]), axis=0)\n",
    "            \n",
    "        return cks\n",
    "    \n",
    "    def distance(self, x1, x2):\n",
    "        \"\"\"Euclidean distance.\"\"\"\n",
    "        #return torch.abs(x1-x2).sum(axis=list(range(1, len(x1.shape)))).reshape(len(x1), 1)\n",
    "        return torch.abs(x1-x2).sum()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward of the model over the data.\"\"\"\n",
    "        xs, cks = inputs\n",
    "        \n",
    "        embs = self.forward_one(xs)\n",
    "        res = []\n",
    "        for emb in embs:\n",
    "            distances = []\n",
    "            for ck in cks:\n",
    "                distances.append(-self.distance(emb, ck))\n",
    "            \n",
    "            res.append(torch.stack(distances))\n",
    "        \n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0006) # initial fixed:  lr=0.0004)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': ReduceLROnPlateau(optimizer, mode='max', factor=0.95, patience=5, threshold=0.0001, min_lr=0.0002),\n",
    "            'monitor': 'val_accuracy'\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        xs, ys = batch\n",
    "        \n",
    "        cks = self.compute_prototypes()\n",
    "        y_hat = self.forward([xs.to(self.device), torch.stack(cks).to(self.device)])\n",
    "        \n",
    "        loss = self.loss(y_hat, torch.tensor(ys, device=self.device))\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        xs, ys = batch\n",
    "\n",
    "        cks = self.compute_prototypes()\n",
    "        y_hat = self.forward([xs.to(self.device), torch.stack(cks).to(self.device)])\n",
    "\n",
    "        val_loss = self.loss(y_hat, torch.tensor(ys, device=self.device))\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "\n",
    "        return {'loss': val_loss, 'preds_strat1': y_hat, 'target': torch.tensor(batch[1], device=self.device)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):  \n",
    "        preds = torch.cat([tmp['preds_strat1'] for tmp in outputs])\n",
    "        targets = torch.cat([tmp['target'] for tmp in outputs])\n",
    "        \n",
    "        # simple metrics\n",
    "        self.log('val_loss', torch.stack([tmp['loss'] for tmp in outputs]).mean())\n",
    "        self.log('val_accuracy', accuracy(preds, targets))\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        xs, ys = batch\n",
    "\n",
    "        cks = self.compute_prototypes()\n",
    "        \n",
    "        y_hat = self.forward([xs.to(self.device), torch.stack(cks).to(self.device)])\n",
    "        test_loss = self.loss(y_hat, torch.tensor(ys, device=self.device))\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "        \n",
    "        return {'loss': test_loss, 'preds_strat1': y_hat, 'target': torch.tensor(batch[1], device=self.device)}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        preds = torch.cat([tmp['preds_strat1'] for tmp in outputs])\n",
    "        targets = torch.cat([tmp['target'] for tmp in outputs])\n",
    "        \n",
    "        # simple metrics\n",
    "        self.log('test_loss', torch.stack([tmp['loss'] for tmp in outputs]).mean())\n",
    "        self.log('test_accuracy', accuracy(preds, targets))\n",
    "        \n",
    "        # confusion matrix\n",
    "        cm = confusion_matrix(preds, targets, num_classes=self.num_classes).cpu().numpy()\n",
    "        normalized_cm = np.around(cm/cm.sum(axis=1)[:, None], 3)*100 # normalize by line\n",
    "\n",
    "        df_cm = pd.DataFrame(normalized_cm, index=self.labels, columns=self.labels)\n",
    "        plt.figure(figsize = (15,12))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "        plt.close(fig_)\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"Test - Confusion matrix\", fig_, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "shots = 10 # number of anchors perclass (for the suport set)\n",
    "queries = 15 # number of query per class\n",
    "\n",
    "# the data\n",
    "data_dir = join(Path.home(), \"data/kaggle_TIMIT\")\n",
    "\n",
    "mfcc_timit = TimitMFCCDataModule(join(Path.home(), \"data/kaggle_TIMIT\"), batch_size, shots, queries=queries)\n",
    "\n",
    "mfcc_timit.prepare_data()\n",
    "mfcc_timit.setup(\"fit\") # to be able to call anchors property\n",
    "\n",
    "keys_timit = [i for i in range(mfcc_timit.timit_gt.phon_size)]\n",
    "labels_timit = [mfcc_timit.timit_gt.index2phn[i] for i in keys_timit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_stats = False\n",
    "if do_stats:\n",
    "    mfcc_timit.setup(\"test\")\n",
    "    \n",
    "    mfcc_timit.df_data_train[\"gt\"] = mfcc_timit.df_data_train[\"gt\"].map(lambda x: labels_timit[x])\n",
    "    mfcc_timit.df_data_test[\"gt\"] = mfcc_timit.df_data_test[\"gt\"].map(lambda x: labels_timit[x])\n",
    "    \n",
    "    print('train stats:')\n",
    "    print(f'number of examples in train: {mfcc_timit.df_data_train[\"gt\"].count()}')\n",
    "    print(f\"number of examples per gt:\\n{mfcc_timit.df_data_train['gt'].value_counts()}\")\n",
    "    print(f\"number of different speakers: {mfcc_timit.df_data_train['speaker_id'].nunique()}\")\n",
    "    print(f\"number of examples per sex:\\n{mfcc_timit.df_data_train['sex'].value_counts()}\")\n",
    "    print(f\"number of examples per region:\\n{mfcc_timit.df_data_train['region'].value_counts()}\")\n",
    "    \n",
    "    print('test stats:')\n",
    "    print(f'number of examples in test: {mfcc_timit.df_data_test[\"gt\"].count()}')\n",
    "    print(f\"number of examples per gt:\\n{mfcc_timit.df_data_test['gt'].value_counts()}\")\n",
    "    print(f\"number of different speakers: {mfcc_timit.df_data_test['speaker_id'].nunique()}\")\n",
    "    print(f\"number of examples per sex:\\n{mfcc_timit.df_data_test['sex'].value_counts()}\")\n",
    "    print(f\"number of examples per region:\\n{mfcc_timit.df_data_test['region'].value_counts()}\")\n",
    "    \n",
    "    # show anchors\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    anchors = {}\n",
    "    for i in labels_timit:\n",
    "        anchors[i] = []\n",
    "        \n",
    "    for anchor in mfcc_timit.anchors:\n",
    "        anchors[labels_timit[anchor[1]]].append(anchor[0].numpy().transpose())\n",
    "        \n",
    "    for anchor in anchors.keys():\n",
    "        fig, ax = plt.subplots(2, 5, figsize=(30,15), sharey=True)\n",
    "        fig.suptitle(f'Anchors {anchor}')\n",
    "        i = 0\n",
    "        for a in anchors[anchor]:\n",
    "            ax[i//5, i%5].imshow(a, origin='lower')\n",
    "            i += 1\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | layer_1_grus  | GRU              | 5.2 M \n",
      "1 | bn_fwd        | BatchNorm1d      | 512   \n",
      "2 | bn_bwd        | BatchNorm1d      | 512   \n",
      "3 | layer_2_dense | Linear           | 65.7 K\n",
      "4 | alpha         | Linear           | 16.4 K\n",
      "5 | loss          | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.081    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid 4\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vroger/.miniconda3/envs/audio_loader/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 18                                                              \n",
      "Epoch 0:   0%|          | 0/18 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vroger/.miniconda3/envs/audio_loader/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  82%|████████▏ | 18/22 [00:05<00:01,  3.48it/s, loss=3.57, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  91%|█████████ | 20/22 [00:05<00:00,  3.54it/s, loss=3.57, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 22/22 [00:06<00:00,  3.32it/s, loss=3.57, v_num=423]\n",
      "Epoch 19:  82%|████████▏ | 18/22 [00:05<00:01,  3.53it/s, loss=2.79, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  91%|█████████ | 20/22 [00:05<00:00,  3.59it/s, loss=2.79, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 22/22 [00:06<00:00,  3.37it/s, loss=2.79, v_num=423]\n",
      "Epoch 29:  82%|████████▏ | 18/22 [00:05<00:01,  3.47it/s, loss=1.69, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  91%|█████████ | 20/22 [00:05<00:00,  3.54it/s, loss=1.69, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 29: 100%|██████████| 22/22 [00:06<00:00,  3.33it/s, loss=1.69, v_num=423]\n",
      "Epoch 39:  82%|████████▏ | 18/22 [00:05<00:01,  3.48it/s, loss=1.31, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39:  91%|█████████ | 20/22 [00:05<00:00,  3.55it/s, loss=1.31, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 39: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=1.31, v_num=423]\n",
      "Epoch 49:  82%|████████▏ | 18/22 [00:05<00:01,  3.57it/s, loss=1.1, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49:  91%|█████████ | 20/22 [00:05<00:00,  3.63it/s, loss=1.1, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 49: 100%|██████████| 22/22 [00:06<00:00,  3.41it/s, loss=1.1, v_num=423]\n",
      "Epoch 59:  82%|████████▏ | 18/22 [00:05<00:01,  3.53it/s, loss=0.644, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.644, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 59: 100%|██████████| 22/22 [00:06<00:00,  3.37it/s, loss=0.644, v_num=423]\n",
      "Epoch 69:  82%|████████▏ | 18/22 [00:05<00:01,  3.52it/s, loss=0.583, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.583, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 69: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.583, v_num=423]\n",
      "Epoch 79:  82%|████████▏ | 18/22 [00:05<00:01,  3.54it/s, loss=1.18, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 79:  91%|█████████ | 20/22 [00:05<00:00,  3.60it/s, loss=1.18, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 79: 100%|██████████| 22/22 [00:06<00:00,  3.38it/s, loss=1.18, v_num=423]\n",
      "Epoch 89:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=0.672, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 89:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.672, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 89: 100%|██████████| 22/22 [00:06<00:00,  3.37it/s, loss=0.672, v_num=423]\n",
      "Epoch 99:  82%|████████▏ | 18/22 [00:05<00:01,  3.52it/s, loss=0.411, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.411, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.411, v_num=423]\n",
      "Epoch 109:  82%|████████▏ | 18/22 [00:05<00:01,  3.46it/s, loss=1.02, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 109:  91%|█████████ | 20/22 [00:05<00:00,  3.52it/s, loss=1.02, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]\u001b[A\n",
      "Epoch 109: 100%|██████████| 22/22 [00:06<00:00,  3.32it/s, loss=1.02, v_num=423]\n",
      "Epoch 119:  82%|████████▏ | 18/22 [00:05<00:01,  3.54it/s, loss=0.306, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 119:  91%|█████████ | 20/22 [00:05<00:00,  3.60it/s, loss=0.306, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 119: 100%|██████████| 22/22 [00:06<00:00,  3.38it/s, loss=0.306, v_num=423]\n",
      "Epoch 129:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.172, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 129:  91%|█████████ | 20/22 [00:05<00:00,  3.55it/s, loss=0.172, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]\u001b[A\n",
      "Epoch 129: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.172, v_num=423]\n",
      "Epoch 139:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.267, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 139:  91%|█████████ | 20/22 [00:05<00:00,  3.54it/s, loss=0.267, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.12it/s]\u001b[A\n",
      "Epoch 139: 100%|██████████| 22/22 [00:06<00:00,  3.32it/s, loss=0.267, v_num=423]\n",
      "Epoch 149:  82%|████████▏ | 18/22 [00:05<00:01,  3.52it/s, loss=0.471, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 149:  91%|█████████ | 20/22 [00:05<00:00,  3.57it/s, loss=0.471, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.13it/s]\u001b[A\n",
      "Epoch 149: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.471, v_num=423]\n",
      "Epoch 159:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.345, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 159:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.345, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]\u001b[A\n",
      "Epoch 159: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.345, v_num=423]\n",
      "Epoch 169:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.353, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 169:  91%|█████████ | 20/22 [00:05<00:00,  3.55it/s, loss=0.353, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 169: 100%|██████████| 22/22 [00:06<00:00,  3.33it/s, loss=0.353, v_num=423]\n",
      "Epoch 179:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.313, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 179:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.313, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 179: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.313, v_num=423]\n",
      "Epoch 189:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=1.14, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 189:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=1.14, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 189: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=1.14, v_num=423]\n",
      "Epoch 199:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.261, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 199:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.261, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.261, v_num=423]\n",
      "Epoch 209:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.201, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 209:  91%|█████████ | 20/22 [00:05<00:00,  3.55it/s, loss=0.201, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 209: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.201, v_num=423]\n",
      "Epoch 219:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.23, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 219:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.23, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 219: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.23, v_num=423]\n",
      "Epoch 229:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.308, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 229:  91%|█████████ | 20/22 [00:05<00:00,  3.55it/s, loss=0.308, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 229: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.308, v_num=423]\n",
      "Epoch 239:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=0.129, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 239:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.129, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 239: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.129, v_num=423]\n",
      "Epoch 249:  82%|████████▏ | 18/22 [00:05<00:01,  3.53it/s, loss=0.234, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 249:  91%|█████████ | 20/22 [00:05<00:00,  3.59it/s, loss=0.234, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 249: 100%|██████████| 22/22 [00:06<00:00,  3.37it/s, loss=0.234, v_num=423]\n",
      "Epoch 259:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=0.646, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 259:  91%|█████████ | 20/22 [00:05<00:00,  3.57it/s, loss=0.646, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 259: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.646, v_num=423]\n",
      "Epoch 269:  82%|████████▏ | 18/22 [00:05<00:01,  3.53it/s, loss=0.469, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 269:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.469, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 269: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.469, v_num=423]\n",
      "Epoch 279:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.23, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 279:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.23, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.16it/s]\u001b[A\n",
      "Epoch 279: 100%|██████████| 22/22 [00:06<00:00,  3.33it/s, loss=0.23, v_num=423]\n",
      "Epoch 289:  82%|████████▏ | 18/22 [00:05<00:01,  3.48it/s, loss=0.219, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 289:  91%|█████████ | 20/22 [00:05<00:00,  3.54it/s, loss=0.219, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 289: 100%|██████████| 22/22 [00:06<00:00,  3.33it/s, loss=0.219, v_num=423]\n",
      "Epoch 299:  82%|████████▏ | 18/22 [00:05<00:01,  3.53it/s, loss=0.075, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 299:  91%|█████████ | 20/22 [00:05<00:00,  3.59it/s, loss=0.075, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.23it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 22/22 [00:06<00:00,  3.38it/s, loss=0.075, v_num=423]\n",
      "Epoch 309:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=0.115, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 309:  91%|█████████ | 20/22 [00:05<00:00,  3.57it/s, loss=0.115, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]\u001b[A\n",
      "Epoch 309: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.115, v_num=423]\n",
      "Epoch 319:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.309, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 319:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.309, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 319: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.309, v_num=423]\n",
      "Epoch 329:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.127, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 329:  91%|█████████ | 20/22 [00:05<00:00,  3.55it/s, loss=0.127, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 329: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.127, v_num=423]\n",
      "Epoch 339:  82%|████████▏ | 18/22 [00:05<00:01,  3.54it/s, loss=0.128, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 339:  91%|█████████ | 20/22 [00:05<00:00,  3.59it/s, loss=0.128, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.14it/s]\u001b[A\n",
      "Epoch 339: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.128, v_num=423]\n",
      "Epoch 349:  82%|████████▏ | 18/22 [00:04<00:01,  3.81it/s, loss=0.165, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 349:  91%|█████████ | 20/22 [00:05<00:00,  3.85it/s, loss=0.165, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 349: 100%|██████████| 22/22 [00:06<00:00,  3.58it/s, loss=0.165, v_num=423]\n",
      "Epoch 359:  82%|████████▏ | 18/22 [00:04<00:01,  3.83it/s, loss=0.527, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 359:  91%|█████████ | 20/22 [00:05<00:00,  3.87it/s, loss=0.527, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 359: 100%|██████████| 22/22 [00:06<00:00,  3.58it/s, loss=0.527, v_num=423]\n",
      "Epoch 369:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=0.339, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 369:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.339, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.16it/s]\u001b[A\n",
      "Epoch 369: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.339, v_num=423]\n",
      "Epoch 379:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=0.0344, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 379:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.0344, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 379: 100%|██████████| 22/22 [00:06<00:00,  3.37it/s, loss=0.0344, v_num=423]\n",
      "Epoch 389:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.139, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 389:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.139, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 389: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.139, v_num=423]\n",
      "Epoch 399:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.076, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 399:  91%|█████████ | 20/22 [00:05<00:00,  3.57it/s, loss=0.076, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 399: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.076, v_num=423]\n",
      "Epoch 409:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=0.166, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 409:  91%|█████████ | 20/22 [00:05<00:00,  3.57it/s, loss=0.166, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]\u001b[A\n",
      "Epoch 409: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.166, v_num=423]\n",
      "Epoch 419:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.247, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 419:  91%|█████████ | 20/22 [00:05<00:00,  3.55it/s, loss=0.247, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 419: 100%|██████████| 22/22 [00:06<00:00,  3.34it/s, loss=0.247, v_num=423]\n",
      "Epoch 429:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.301, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 429:  91%|█████████ | 20/22 [00:05<00:00,  3.55it/s, loss=0.301, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 429: 100%|██████████| 22/22 [00:06<00:00,  3.33it/s, loss=0.301, v_num=423]\n",
      "Epoch 439:  82%|████████▏ | 18/22 [00:05<00:01,  3.53it/s, loss=0.187, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 439:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.187, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 439: 100%|██████████| 22/22 [00:06<00:00,  3.37it/s, loss=0.187, v_num=423]\n",
      "Epoch 449:  82%|████████▏ | 18/22 [00:05<00:01,  3.52it/s, loss=0.0583, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 449:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.0583, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 449: 100%|██████████| 22/22 [00:06<00:00,  3.37it/s, loss=0.0583, v_num=423]\n",
      "Epoch 459:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.163, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 459:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.163, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 459: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.163, v_num=423]\n",
      "Epoch 469:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.14, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 469:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.14, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.24it/s]\u001b[A\n",
      "Epoch 469: 100%|██████████| 22/22 [00:06<00:00,  3.35it/s, loss=0.14, v_num=423]\n",
      "Epoch 479:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.116, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 479:  91%|█████████ | 20/22 [00:05<00:00,  3.56it/s, loss=0.116, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.23it/s]\u001b[A\n",
      "Epoch 479: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.116, v_num=423]\n",
      "Epoch 489:  82%|████████▏ | 18/22 [00:05<00:01,  3.53it/s, loss=0.271, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 489:  91%|█████████ | 20/22 [00:05<00:00,  3.60it/s, loss=0.271, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.24it/s]\u001b[A\n",
      "Epoch 489: 100%|██████████| 22/22 [00:06<00:00,  3.39it/s, loss=0.271, v_num=423]\n",
      "Epoch 499:  82%|████████▏ | 18/22 [00:05<00:01,  3.49it/s, loss=0.133, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 499:  91%|█████████ | 20/22 [00:05<00:00,  3.54it/s, loss=0.133, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 499: 100%|██████████| 22/22 [00:06<00:00,  3.32it/s, loss=0.133, v_num=423]\n",
      "Epoch 509:  82%|████████▏ | 18/22 [00:05<00:01,  3.54it/s, loss=0.108, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 509:  91%|█████████ | 20/22 [00:05<00:00,  3.60it/s, loss=0.108, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 509: 100%|██████████| 22/22 [00:06<00:00,  3.38it/s, loss=0.108, v_num=423]\n",
      "Epoch 519:  82%|████████▏ | 18/22 [00:05<00:01,  3.50it/s, loss=0.213, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 519:  91%|█████████ | 20/22 [00:05<00:00,  3.57it/s, loss=0.213, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 519: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.213, v_num=423]\n",
      "Epoch 529:  82%|████████▏ | 18/22 [00:05<00:01,  3.52it/s, loss=0.145, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 529:  91%|█████████ | 20/22 [00:05<00:00,  3.59it/s, loss=0.145, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.23it/s]\u001b[A\n",
      "Epoch 529: 100%|██████████| 22/22 [00:06<00:00,  3.37it/s, loss=0.145, v_num=423]\n",
      "Epoch 539:  82%|████████▏ | 18/22 [00:05<00:01,  3.52it/s, loss=0.126, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 539:  91%|█████████ | 20/22 [00:05<00:00,  3.58it/s, loss=0.126, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 539: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.126, v_num=423]\n",
      "Epoch 549:  82%|████████▏ | 18/22 [00:05<00:01,  3.51it/s, loss=0.0203, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 549:  91%|█████████ | 20/22 [00:05<00:00,  3.57it/s, loss=0.0203, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 549: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, loss=0.0203, v_num=423]\n",
      "Epoch 559:  82%|████████▏ | 18/22 [00:04<00:01,  3.84it/s, loss=0.0437, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 559:  91%|█████████ | 20/22 [00:05<00:00,  3.88it/s, loss=0.0437, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.23it/s]\u001b[A\n",
      "Epoch 559: 100%|██████████| 22/22 [00:06<00:00,  3.61it/s, loss=0.0437, v_num=423]\n",
      "Epoch 569:  82%|████████▏ | 18/22 [00:04<00:01,  3.85it/s, loss=0.0412, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 569:  91%|█████████ | 20/22 [00:05<00:00,  3.89it/s, loss=0.0412, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 569: 100%|██████████| 22/22 [00:06<00:00,  3.61it/s, loss=0.0412, v_num=423]\n",
      "Epoch 579:  82%|████████▏ | 18/22 [00:04<00:01,  3.85it/s, loss=0.228, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 579:  91%|█████████ | 20/22 [00:05<00:00,  3.89it/s, loss=0.228, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 579: 100%|██████████| 22/22 [00:06<00:00,  3.62it/s, loss=0.228, v_num=423]\n",
      "Epoch 589:  82%|████████▏ | 18/22 [00:04<00:01,  3.86it/s, loss=0.157, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 589:  91%|█████████ | 20/22 [00:05<00:00,  3.90it/s, loss=0.157, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 589: 100%|██████████| 22/22 [00:06<00:00,  3.62it/s, loss=0.157, v_num=423]\n",
      "Epoch 599:  82%|████████▏ | 18/22 [00:04<00:01,  3.84it/s, loss=0.0749, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 599:  91%|█████████ | 20/22 [00:05<00:00,  3.89it/s, loss=0.0749, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.23it/s]\u001b[A\n",
      "Epoch 599: 100%|██████████| 22/22 [00:06<00:00,  3.61it/s, loss=0.0749, v_num=423]\n",
      "Epoch 609:  82%|████████▏ | 18/22 [00:04<00:01,  3.85it/s, loss=0.0973, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 609:  91%|█████████ | 20/22 [00:05<00:00,  3.90it/s, loss=0.0973, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.23it/s]\u001b[A\n",
      "Epoch 609: 100%|██████████| 22/22 [00:06<00:00,  3.62it/s, loss=0.0973, v_num=423]\n",
      "Epoch 619:  82%|████████▏ | 18/22 [00:04<00:01,  3.84it/s, loss=0.0468, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 619:  91%|█████████ | 20/22 [00:05<00:00,  3.89it/s, loss=0.0468, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.24it/s]\u001b[A\n",
      "Epoch 619: 100%|██████████| 22/22 [00:06<00:00,  3.61it/s, loss=0.0468, v_num=423]\n",
      "Epoch 629:  82%|████████▏ | 18/22 [00:05<00:01,  3.43it/s, loss=0.069, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 629:  91%|█████████ | 20/22 [00:05<00:00,  3.49it/s, loss=0.069, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 629: 100%|██████████| 22/22 [00:06<00:00,  3.29it/s, loss=0.069, v_num=423]\n",
      "Epoch 639:  82%|████████▏ | 18/22 [00:04<00:01,  3.65it/s, loss=0.178, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 639:  91%|█████████ | 20/22 [00:05<00:00,  3.71it/s, loss=0.178, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 639: 100%|██████████| 22/22 [00:06<00:00,  3.47it/s, loss=0.178, v_num=423]\n",
      "Epoch 649:  82%|████████▏ | 18/22 [00:05<00:01,  3.43it/s, loss=0.111, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 649:  91%|█████████ | 20/22 [00:05<00:00,  3.50it/s, loss=0.111, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 649: 100%|██████████| 22/22 [00:06<00:00,  3.29it/s, loss=0.111, v_num=423]\n",
      "Epoch 659:  82%|████████▏ | 18/22 [00:05<00:01,  3.42it/s, loss=0.0725, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 659:  91%|█████████ | 20/22 [00:05<00:00,  3.49it/s, loss=0.0725, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]\u001b[A\n",
      "Epoch 659: 100%|██████████| 22/22 [00:06<00:00,  3.28it/s, loss=0.0725, v_num=423]\n",
      "Epoch 669:  82%|████████▏ | 18/22 [00:05<00:01,  3.34it/s, loss=0.098, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 669:  91%|█████████ | 20/22 [00:05<00:00,  3.41it/s, loss=0.098, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]\u001b[A\n",
      "Epoch 669: 100%|██████████| 22/22 [00:06<00:00,  3.22it/s, loss=0.098, v_num=423]\n",
      "Epoch 679:  82%|████████▏ | 18/22 [00:05<00:01,  3.27it/s, loss=0.0376, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 679:  91%|█████████ | 20/22 [00:05<00:00,  3.34it/s, loss=0.0376, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.14it/s]\u001b[A\n",
      "Epoch 679: 100%|██████████| 22/22 [00:06<00:00,  3.16it/s, loss=0.0376, v_num=423]\n",
      "Epoch 689:  82%|████████▏ | 18/22 [00:05<00:01,  3.39it/s, loss=0.0466, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 689:  91%|█████████ | 20/22 [00:05<00:00,  3.47it/s, loss=0.0466, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 689: 100%|██████████| 22/22 [00:06<00:00,  3.27it/s, loss=0.0466, v_num=423]\n",
      "Epoch 699:  82%|████████▏ | 18/22 [00:05<00:01,  3.27it/s, loss=0.0457, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 699:  91%|█████████ | 20/22 [00:05<00:00,  3.34it/s, loss=0.0457, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 699: 100%|██████████| 22/22 [00:06<00:00,  3.17it/s, loss=0.0457, v_num=423]\n",
      "Epoch 709:  82%|████████▏ | 18/22 [00:05<00:01,  3.37it/s, loss=0.0212, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 709:  91%|█████████ | 20/22 [00:05<00:00,  3.45it/s, loss=0.0212, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 709: 100%|██████████| 22/22 [00:06<00:00,  3.25it/s, loss=0.0212, v_num=423]\n",
      "Epoch 719:  82%|████████▏ | 18/22 [00:05<00:01,  3.22it/s, loss=0.0914, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 719:  91%|█████████ | 20/22 [00:06<00:00,  3.30it/s, loss=0.0914, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 719: 100%|██████████| 22/22 [00:07<00:00,  3.13it/s, loss=0.0914, v_num=423]\n",
      "Epoch 729:  82%|████████▏ | 18/22 [00:05<00:01,  3.27it/s, loss=0.0708, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 729:  91%|█████████ | 20/22 [00:05<00:00,  3.35it/s, loss=0.0708, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 729: 100%|██████████| 22/22 [00:06<00:00,  3.18it/s, loss=0.0708, v_num=423]\n",
      "Epoch 739:  82%|████████▏ | 18/22 [00:05<00:01,  3.24it/s, loss=0.173, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 739:  91%|█████████ | 20/22 [00:06<00:00,  3.32it/s, loss=0.173, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]\u001b[A\n",
      "Epoch 739: 100%|██████████| 22/22 [00:06<00:00,  3.15it/s, loss=0.173, v_num=423]\n",
      "Epoch 749:  82%|████████▏ | 18/22 [00:05<00:01,  3.33it/s, loss=0.0839, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 749:  91%|█████████ | 20/22 [00:05<00:00,  3.40it/s, loss=0.0839, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 749: 100%|██████████| 22/22 [00:06<00:00,  3.22it/s, loss=0.0839, v_num=423]\n",
      "Epoch 759:  82%|████████▏ | 18/22 [00:05<00:01,  3.26it/s, loss=0.0279, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 759:  91%|█████████ | 20/22 [00:06<00:00,  3.33it/s, loss=0.0279, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.14it/s]\u001b[A\n",
      "Epoch 759: 100%|██████████| 22/22 [00:06<00:00,  3.15it/s, loss=0.0279, v_num=423]\n",
      "Epoch 769:  82%|████████▏ | 18/22 [00:05<00:01,  3.29it/s, loss=0.0467, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 769:  91%|█████████ | 20/22 [00:05<00:00,  3.36it/s, loss=0.0467, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.16it/s]\u001b[A\n",
      "Epoch 769: 100%|██████████| 22/22 [00:06<00:00,  3.19it/s, loss=0.0467, v_num=423]\n",
      "Epoch 779:  82%|████████▏ | 18/22 [00:05<00:01,  3.24it/s, loss=0.175, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 779:  91%|█████████ | 20/22 [00:06<00:00,  3.32it/s, loss=0.175, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 779: 100%|██████████| 22/22 [00:06<00:00,  3.14it/s, loss=0.175, v_num=423]\n",
      "Epoch 789:  82%|████████▏ | 18/22 [00:05<00:01,  3.25it/s, loss=0.148, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 789:  91%|█████████ | 20/22 [00:06<00:00,  3.33it/s, loss=0.148, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 789: 100%|██████████| 22/22 [00:06<00:00,  3.16it/s, loss=0.148, v_num=423]\n",
      "Epoch 799:  82%|████████▏ | 18/22 [00:05<00:01,  3.22it/s, loss=0.0442, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 799:  91%|█████████ | 20/22 [00:06<00:00,  3.29it/s, loss=0.0442, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.13it/s]\u001b[A\n",
      "Epoch 799: 100%|██████████| 22/22 [00:07<00:00,  3.11it/s, loss=0.0442, v_num=423]\n",
      "Epoch 809:  82%|████████▏ | 18/22 [00:05<00:01,  3.27it/s, loss=0.0507, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 809:  91%|█████████ | 20/22 [00:05<00:00,  3.35it/s, loss=0.0507, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 809: 100%|██████████| 22/22 [00:06<00:00,  3.18it/s, loss=0.0507, v_num=423]\n",
      "Epoch 819:  82%|████████▏ | 18/22 [00:05<00:01,  3.25it/s, loss=0.0793, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 819:  91%|█████████ | 20/22 [00:06<00:00,  3.32it/s, loss=0.0793, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.14it/s]\u001b[A\n",
      "Epoch 819: 100%|██████████| 22/22 [00:06<00:00,  3.15it/s, loss=0.0793, v_num=423]\n",
      "Epoch 829:  82%|████████▏ | 18/22 [00:05<00:01,  3.28it/s, loss=0.0274, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 829:  91%|█████████ | 20/22 [00:05<00:00,  3.36it/s, loss=0.0274, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Epoch 829: 100%|██████████| 22/22 [00:06<00:00,  3.19it/s, loss=0.0274, v_num=423]\n",
      "Epoch 839:  82%|████████▏ | 18/22 [00:05<00:01,  3.22it/s, loss=0.0758, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 839:  91%|█████████ | 20/22 [00:06<00:00,  3.30it/s, loss=0.0758, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]\u001b[A\n",
      "Epoch 839: 100%|██████████| 22/22 [00:07<00:00,  3.13it/s, loss=0.0758, v_num=423]\n",
      "Epoch 849:  82%|████████▏ | 18/22 [00:05<00:01,  3.11it/s, loss=0.0722, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 849:  91%|█████████ | 20/22 [00:06<00:00,  3.20it/s, loss=0.0722, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.13it/s]\u001b[A\n",
      "Epoch 849: 100%|██████████| 22/22 [00:07<00:00,  3.04it/s, loss=0.0722, v_num=423]\n",
      "Epoch 859:  82%|████████▏ | 18/22 [00:05<00:01,  3.07it/s, loss=0.00568, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 859:  91%|█████████ | 20/22 [00:06<00:00,  3.15it/s, loss=0.00568, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.15it/s]\u001b[A\n",
      "Epoch 859: 100%|██████████| 22/22 [00:07<00:00,  2.99it/s, loss=0.00568, v_num=423]\n",
      "Epoch 869:  82%|████████▏ | 18/22 [00:06<00:01,  2.92it/s, loss=0.0244, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 869:  91%|█████████ | 20/22 [00:06<00:00,  3.00it/s, loss=0.0244, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.09it/s]\u001b[A\n",
      "Epoch 869: 100%|██████████| 22/22 [00:07<00:00,  2.87it/s, loss=0.0244, v_num=423]\n",
      "Epoch 879:  82%|████████▏ | 18/22 [00:06<00:01,  2.97it/s, loss=0.0115, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 879:  91%|█████████ | 20/22 [00:06<00:00,  3.06it/s, loss=0.0115, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.11it/s]\u001b[A\n",
      "Epoch 879: 100%|██████████| 22/22 [00:07<00:00,  2.91it/s, loss=0.0115, v_num=423]\n",
      "Epoch 889:  82%|████████▏ | 18/22 [00:06<00:01,  2.66it/s, loss=0.0125, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 889:  91%|█████████ | 20/22 [00:07<00:00,  2.67it/s, loss=0.0125, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s]\u001b[A\n",
      "Epoch 889: 100%|██████████| 22/22 [00:09<00:00,  2.43it/s, loss=0.0125, v_num=423]\n",
      "Epoch 899:  82%|████████▏ | 18/22 [00:06<00:01,  2.87it/s, loss=0.0342, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 899:  91%|█████████ | 20/22 [00:06<00:00,  2.95it/s, loss=0.0342, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:01,  2.00it/s]\u001b[A\n",
      "Epoch 899: 100%|██████████| 22/22 [00:07<00:00,  2.80it/s, loss=0.0342, v_num=423]\n",
      "Epoch 909:  82%|████████▏ | 18/22 [00:06<00:01,  2.67it/s, loss=0.0421, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 909:  91%|█████████ | 20/22 [00:07<00:00,  2.67it/s, loss=0.0421, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s]\u001b[A\n",
      "Epoch 909: 100%|██████████| 22/22 [00:09<00:00,  2.44it/s, loss=0.0421, v_num=423]\n",
      "Epoch 919:  82%|████████▏ | 18/22 [00:06<00:01,  2.89it/s, loss=0.0026, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 919:  91%|█████████ | 20/22 [00:06<00:00,  2.97it/s, loss=0.0026, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.02it/s]\u001b[A\n",
      "Epoch 919: 100%|██████████| 22/22 [00:07<00:00,  2.82it/s, loss=0.0026, v_num=423]\n",
      "Epoch 929:  82%|████████▏ | 18/22 [00:06<00:01,  2.99it/s, loss=0.0595, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 929:  91%|█████████ | 20/22 [00:06<00:00,  3.06it/s, loss=0.0595, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.06it/s]\u001b[A\n",
      "Epoch 929: 100%|██████████| 22/22 [00:07<00:00,  2.92it/s, loss=0.0595, v_num=423]\n",
      "Epoch 939:  82%|████████▏ | 18/22 [00:06<00:01,  2.84it/s, loss=0.106, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 939:  91%|█████████ | 20/22 [00:06<00:00,  2.92it/s, loss=0.106, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.03it/s]\u001b[A\n",
      "Epoch 939: 100%|██████████| 22/22 [00:07<00:00,  2.78it/s, loss=0.106, v_num=423]\n",
      "Epoch 949:  82%|████████▏ | 18/22 [00:06<00:01,  2.85it/s, loss=0.0494, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 949:  91%|█████████ | 20/22 [00:06<00:00,  2.93it/s, loss=0.0494, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.01it/s]\u001b[A\n",
      "Epoch 949: 100%|██████████| 22/22 [00:07<00:00,  2.80it/s, loss=0.0494, v_num=423]\n",
      "Epoch 959:  82%|████████▏ | 18/22 [00:06<00:01,  2.98it/s, loss=0.0126, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 959:  91%|█████████ | 20/22 [00:06<00:00,  3.06it/s, loss=0.0126, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.04it/s]\u001b[A\n",
      "Epoch 959: 100%|██████████| 22/22 [00:07<00:00,  2.90it/s, loss=0.0126, v_num=423]\n",
      "Epoch 969:  82%|████████▏ | 18/22 [00:06<00:01,  3.00it/s, loss=0.0998, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 969:  91%|█████████ | 20/22 [00:06<00:00,  3.06it/s, loss=0.0998, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.02it/s]\u001b[A\n",
      "Epoch 969: 100%|██████████| 22/22 [00:07<00:00,  2.90it/s, loss=0.0998, v_num=423]\n",
      "Epoch 979:  82%|████████▏ | 18/22 [00:06<00:01,  2.85it/s, loss=0.0482, v_num=423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 979:  91%|█████████ | 20/22 [00:06<00:00,  2.93it/s, loss=0.0482, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:01<00:01,  1.96it/s]\u001b[A\n",
      "Epoch 979: 100%|██████████| 22/22 [00:07<00:00,  2.78it/s, loss=0.0482, v_num=423]\n",
      "Epoch 989:  82%|████████▏ | 18/22 [00:06<00:01,  2.86it/s, loss=0.0309, v_num=423] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 989:  91%|█████████ | 20/22 [00:06<00:00,  2.93it/s, loss=0.0309, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "Epoch 989: 100%|██████████| 22/22 [00:07<00:00,  2.79it/s, loss=0.0309, v_num=423]\n",
      "Epoch 999:  82%|████████▏ | 18/22 [00:06<00:01,  2.97it/s, loss=0.0123, v_num=423]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 999:  91%|█████████ | 20/22 [00:06<00:00,  3.04it/s, loss=0.0123, v_num=423]\n",
      "Validating:  50%|█████     | 2/4 [00:00<00:00,  2.04it/s]\u001b[A\n",
      "Epoch 999: 100%|██████████| 22/22 [00:07<00:00,  2.89it/s, loss=0.0123, v_num=423]\n",
      "Epoch 999: 100%|██████████| 22/22 [00:07<00:00,  2.89it/s, loss=0.0123, v_num=423]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init model\n",
    "model = lit_mfcc_model(13*3, mfcc_timit.anchors, mfcc_timit.timit_gt.phon_size, shots, labels_timit) # features = MFCC + delta+ deltas deltas\n",
    "model.to('cuda')\n",
    "\n",
    "# trainer definition\n",
    "trainer = pl.Trainer(\n",
    "     callbacks=[\n",
    "#        EarlyStopping(monitor='val_accuracy_argmin', patience=200, mode=\"max\"),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ],\n",
    "    checkpoint_callback=ModelCheckpoint(save_top_k=5, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "    progress_bar_refresh_rate=None,\n",
    "    gpus=1, auto_select_gpus=True,\n",
    "    check_val_every_n_epoch=10,\n",
    "    precision=16,\n",
    "    max_epochs=1000\n",
    ")\n",
    "\n",
    "trainer.fit(model, mfcc_timit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(\n",
    "#    callbacks=[\n",
    "# #        EarlyStopping(monitor='val_accuracy_argmin', patience=200, mode=\"max\"),\n",
    "#        LearningRateMonitor(logging_interval='epoch')\n",
    "#    ],\n",
    "#    checkpoint_callback=ModelCheckpoint(save_top_k=5, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "#    progress_bar_refresh_rate=None,\n",
    "#    gpus=1, auto_select_gpus=True,\n",
    "#    check_val_every_n_epoch=5,\n",
    "#    precision=16,\n",
    "#    max_epochs=2000\n",
    "#)\n",
    "# trainer.fit(model, mfcc_timit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not do_stats:\n",
    "    mfcc_timit.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 218\n",
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vroger/.miniconda3/envs/audio_loader/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 218/218 [01:49<00:00,  1.99it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': 0.3975563645362854, 'test_loss': 14.415218353271484}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 14.415218353271484, 'test_accuracy': 0.3975563645362854}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(test_dataloaders=mfcc_timit.test_dataloader(), ckpt_path='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
