{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.metrics.functional import accuracy, confusion_matrix\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "from audio_loader.features.raw_audio import WindowedAudio\n",
    "from audio_loader.features.mfcc import WindowedMFCC\n",
    "from audio_loader.ground_truth.timit import TimitGroundTruth\n",
    "from audio_loader.samplers.dynamic_sampler import DynamicSampler\n",
    "from audio_loader.dl_frontends.pytorch.fill_ram import get_dataset_dynamic_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimitMFCCDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, batch_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.timit_gt = TimitGroundTruth(self.data_dir, phon_class=\"phon_class2\", with_silences=False)\n",
    "        self.mfcc_feature_processor = WindowedMFCC(400, 160, 16000, 13, delta_orders=[1, 2], delta_width=9)\n",
    "        self.mfcc_sampler = DynamicSampler([self.mfcc_feature_processor], self.timit_gt)\n",
    "        self.original_train_dataset, self.collate_func = get_dataset_dynamic_size(self.mfcc_sampler, \"train\")\n",
    "        self.test_dataset, self.collate_func = get_dataset_dynamic_size(self.mfcc_sampler, \"test\")\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.val_nb_samples = round(len(self.original_train_dataset)/100)\n",
    "            self.train_nb_samples = len(self.original_train_dataset) - self.val_nb_samples\n",
    "            self.train_dataset, self.val_dataset = random_split(\n",
    "                self.original_train_dataset,\n",
    "                [self.train_nb_samples, self.val_nb_samples]\n",
    "            )\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            return\n",
    "            \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        print(f\"Train number of examples: {len(self.train_dataset)}\\n\")\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                          collate_fn=self.collate_func,\n",
    "                          drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        #return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "        #                  collate_fn=self.collate_func,\n",
    "        #                  drop_last=False)\n",
    "        return DataLoader(self.test_dataset, self.batch_size , shuffle=False,\n",
    "                          collate_fn=self.collate_func,\n",
    "                          drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, self.batch_size , shuffle=False,\n",
    "                          collate_fn=self.collate_func,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data\n",
    "mfcc_timit = TimitMFCCDataModule(join(Path.home(), \"data/kaggle_TIMIT\"), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lit_mfcc_model(pl.LightningModule):\n",
    "    def __init__(self, feature_size, labels):\n",
    "        \"\"\"Init all parameters.\n",
    "        \n",
    "        feature_size: int\n",
    "            size of the expected features for the forward step\n",
    "        labels: list\n",
    "            Ground truth labels to display in the confusion matrix\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.layer_1_grus = nn.GRU(\n",
    "            feature_size, 550, 5,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        self.bn_fwd = nn.BatchNorm1d(550)\n",
    "        self.bn_bwd = nn.BatchNorm1d(550)\n",
    "        self.layer_2_dense = torch.nn.Linear(1100, 128)\n",
    "        self.bn_layer_2 = nn.BatchNorm1d(128)\n",
    "        self.act_layer_2 = nn.LeakyReLU(0.1) # in pytorch kaldi it is softmax\n",
    "        \n",
    "        self.layer_3_dense = torch.nn.Linear(128, 58)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward of the model over the data.\"\"\"\n",
    "        batch_size = x.batch_sizes[0]\n",
    "        # shape: (num_layers*directions, batch_size, hidden_size?)\n",
    "        h_0 = torch.zeros(5*2, batch_size, 550, device=\"cuda\")\n",
    "        output, h_n = self.layer_1_grus(x, h_0)\n",
    "\n",
    "        fwd_h = h_n.view(5, 2, batch_size, 550)[-1, 0]\n",
    "        bwd_h = h_n.view(5, 2, batch_size, 550)[-1, 1]\n",
    "    \n",
    "        fwd_h = self.bn_fwd(fwd_h.view(batch_size, 550))\n",
    "        bwd_h = self.bn_bwd(bwd_h.view(batch_size, 550))\n",
    "\n",
    "        h = torch.cat((fwd_h, bwd_h), 1)\n",
    "        dense1 = self.bn_layer_2(self.act_layer_2(self.layer_2_dense(h)))\n",
    "        return self.layer_3_dense(dense1)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0004)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x.to('cuda'))\n",
    "        _, y = torch.stack(y).max(dim=1)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x.to('cuda'))\n",
    "        _, target = torch.stack(y).max(dim=1)\n",
    "        _, pred = y_hat.max(dim=1)\n",
    "\n",
    "        val_loss = F.cross_entropy(y_hat, target)\n",
    "        return {'loss': val_loss, 'preds_strat1': pred, 'target': target}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):  \n",
    "        preds = torch.cat([tmp['preds_strat1'] for tmp in outputs])\n",
    "        targets = torch.cat([tmp['target'] for tmp in outputs])\n",
    "        \n",
    "        # simple metrics\n",
    "        self.log('val_loss', torch.stack([tmp['loss'] for tmp in outputs]).mean())\n",
    "        self.log('val_accuracy', accuracy(preds, targets))\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x.to('cuda'))\n",
    "        _, target = torch.stack(y).max(dim=1)\n",
    "        _, pred = y_hat.max(dim=1)\n",
    "        \n",
    "        test_loss = F.cross_entropy(y_hat, target)\n",
    "        return {'loss': test_loss, 'preds_strat1': pred, 'target': target}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        preds = torch.cat([tmp['preds_strat1'] for tmp in outputs])\n",
    "        targets = torch.cat([tmp['target'] for tmp in outputs])\n",
    "        \n",
    "        # simple metrics\n",
    "        self.log('test_loss', torch.stack([tmp['loss'] for tmp in outputs]).mean())\n",
    "        self.log('test_accuracy', accuracy(preds, targets))\n",
    "        \n",
    "        # confusion matrix\n",
    "        num_classes = len(self.labels)\n",
    "        \n",
    "        cm = confusion_matrix(preds, targets, num_classes=num_classes).cpu().numpy()\n",
    "        normalized_cm = np.around(cm/cm.sum(axis=1)[:, None], 3)*100 # normalize by line\n",
    "\n",
    "        df_cm = pd.DataFrame(normalized_cm, index=self.labels, columns=self.labels)\n",
    "        plt.figure(figsize = (15,12))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "        plt.close(fig_)\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"Test - Confusion matrix\", fig_, self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lit_mfcc_model(\n",
       "  (layer_1_grus): GRU(39, 550, num_layers=5, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (bn_fwd): BatchNorm1d(550, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_bwd): BatchNorm1d(550, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer_2_dense): Linear(in_features=1100, out_features=128, bias=True)\n",
       "  (bn_layer_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act_layer_2): LeakyReLU(negative_slope=0.1)\n",
       "  (layer_3_dense): Linear(in_features=128, out_features=58, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels\n",
    "mfcc_timit.prepare_data()\n",
    "keys_timit = [i for i in range(mfcc_timit.timit_gt.phon_size)]\n",
    "labels = [mfcc_timit.timit_gt.index2phn[i] for i in keys_timit]\n",
    "\n",
    "# init model\n",
    "model = lit_mfcc_model(13*3, labels) # MFCC + delta+ deltas deltas\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "# trainer definition\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=10, mode=\"min\")\n",
    "    ],\n",
    "    checkpoint_callback=ModelCheckpoint(save_top_k=5, monitor=\"val_loss\", mode=\"min\"),\n",
    "    progress_bar_refresh_rate=1000,\n",
    "    gpus=1, auto_select_gpus=True,\n",
    "    precision=16,\n",
    "    max_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type        | Params\n",
      "----------------------------------------------\n",
      "0 | layer_1_grus  | GRU         | 23.8 M\n",
      "1 | bn_fwd        | BatchNorm1d | 1.1 K \n",
      "2 | bn_bwd        | BatchNorm1d | 1.1 K \n",
      "3 | layer_2_dense | Linear      | 140 K \n",
      "4 | bn_layer_2    | BatchNorm1d | 256   \n",
      "5 | act_layer_2   | LeakyReLU   | 0     \n",
      "6 | layer_3_dense | Linear      | 7.5 K \n",
      "----------------------------------------------\n",
      "23.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.9 M    Total params\n",
      "95.630    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train number of examples: 139791                              \n",
      "\n",
      "Epoch 0:   0%|          | 0/11947 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vroger/.miniconda3/envs/audio_loader/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/vroger/.miniconda3/envs/audio_loader/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  75%|███████▌  | 9000/11947 [06:20<02:04, 23.67it/s, loss=1.28, v_num=469]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  84%|████████▎ | 10000/11947 [06:26<01:15, 25.85it/s, loss=1.28, v_num=469]\n",
      "Epoch 0:  92%|█████████▏| 11000/11947 [06:33<00:33, 27.95it/s, loss=1.28, v_num=469]\n",
      "Validating:  93%|█████████▎| 3000/3211 [00:19<00:01, 150.51it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 11947/11947 [06:41<00:00, 29.75it/s, loss=1.21, v_num=469]\n",
      "Epoch 1:  75%|███████▌  | 9000/11947 [04:47<01:34, 31.33it/s, loss=0.979, v_num=469]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  84%|████████▎ | 10000/11947 [04:53<00:57, 34.02it/s, loss=0.979, v_num=469]\n",
      "Epoch 1:  92%|█████████▏| 11000/11947 [05:00<00:25, 36.59it/s, loss=0.979, v_num=469]\n",
      "Validating:  93%|█████████▎| 3000/3211 [00:19<00:01, 150.41it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 11947/11947 [05:08<00:00, 38.71it/s, loss=1.05, v_num=469] \n",
      "Epoch 2:  75%|███████▌  | 9000/11947 [06:43<02:12, 22.30it/s, loss=0.947, v_num=469]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  84%|████████▎ | 10000/11947 [06:50<01:19, 24.37it/s, loss=0.947, v_num=469]\n",
      "Epoch 2:  92%|█████████▏| 11000/11947 [06:56<00:35, 26.38it/s, loss=0.947, v_num=469]\n",
      "Validating:  93%|█████████▎| 3000/3211 [00:20<00:01, 150.03it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 11947/11947 [07:04<00:00, 28.11it/s, loss=1.1, v_num=469]  \n",
      "Epoch 3:  75%|███████▌  | 9000/11947 [05:34<01:49, 26.89it/s, loss=1.05, v_num=469] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  84%|████████▎ | 10000/11947 [05:41<01:06, 29.30it/s, loss=1.05, v_num=469]\n",
      "Epoch 3:  92%|█████████▏| 11000/11947 [05:47<00:29, 31.61it/s, loss=1.05, v_num=469]\n",
      "Validating:  93%|█████████▎| 3000/3211 [00:19<00:01, 150.26it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 11947/11947 [05:56<00:00, 33.56it/s, loss=1.04, v_num=469]\n",
      "Epoch 4:  75%|███████▌  | 9000/11947 [06:43<02:12, 22.32it/s, loss=0.889, v_num=469]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  84%|████████▎ | 10000/11947 [06:49<01:19, 24.40it/s, loss=0.889, v_num=469]\n",
      "Epoch 4:  92%|█████████▏| 11000/11947 [06:56<00:35, 26.41it/s, loss=0.889, v_num=469]\n",
      "Validating:  93%|█████████▎| 3000/3211 [00:20<00:01, 149.96it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 11947/11947 [07:04<00:00, 28.14it/s, loss=0.951, v_num=469]\n",
      "Epoch 5:  75%|███████▌  | 9000/11947 [06:44<02:12, 22.23it/s, loss=0.949, v_num=469] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  84%|████████▎ | 10000/11947 [06:51<01:20, 24.30it/s, loss=0.949, v_num=469]\n",
      "Epoch 5:  92%|█████████▏| 11000/11947 [06:58<00:36, 26.30it/s, loss=0.949, v_num=469]\n",
      "Validating:  93%|█████████▎| 3000/3211 [00:20<00:01, 149.40it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 11947/11947 [07:06<00:00, 28.02it/s, loss=0.914, v_num=469]\n",
      "Epoch 6:  75%|███████▌  | 9000/11947 [06:44<02:12, 22.23it/s, loss=0.858, v_num=469] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  84%|████████▎ | 10000/11947 [06:51<01:20, 24.30it/s, loss=0.858, v_num=469]\n",
      "Epoch 6:  92%|█████████▏| 11000/11947 [06:58<00:36, 26.30it/s, loss=0.858, v_num=469]\n",
      "Validating:  93%|█████████▎| 3000/3211 [00:20<00:01, 149.71it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 11947/11947 [07:06<00:00, 28.03it/s, loss=0.848, v_num=469]\n",
      "Epoch 7:  75%|███████▌  | 9000/11947 [06:41<02:11, 22.39it/s, loss=0.905, v_num=469] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  84%|████████▎ | 10000/11947 [06:48<01:19, 24.47it/s, loss=0.905, v_num=469]\n",
      "Epoch 7:  92%|█████████▏| 11000/11947 [06:55<00:35, 26.48it/s, loss=0.905, v_num=469]\n",
      "Validating:  93%|█████████▎| 3000/3211 [00:20<00:01, 149.42it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 11947/11947 [07:03<00:00, 28.22it/s, loss=0.941, v_num=469]\n",
      "Epoch 8:  50%|█████     | 6000/11947 [04:50<04:48, 20.62it/s, loss=0.831, v_num=469] "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, mfcc_timit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, mfcc_timit.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
